{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: ../misc/corefBias/WinoBias/wino/data/pro_stereotyped_type2.txt.test\n",
      "101\n",
      "The librarian talked to [the carpenter] and asked [him] to fix the table in the library.\n"
     ]
    }
   ],
   "source": [
    "files: list = [\n",
    "    # \"anti_stereotyped_type1.txt.dev\",\n",
    "    # \"anti_stereotyped_type1.txt.test\",\n",
    "    # \"anti_stereotyped_type2.txt.dev\",\n",
    "    # \"anti_stereotyped_type2.txt.test\",\n",
    "    # \"female_occupations.txt\",\n",
    "    # \"male_occupations.txt\",\n",
    "    # \"pro_stereotyped_type1.txt.dev\",\n",
    "    # \"pro_stereotyped_type1.txt.test\",\n",
    "    # \"pro_stereotyped_type2.txt.dev\",\n",
    "    # \"pro_stereotyped_type2.txt.test\"\n",
    "]\n",
    "\n",
    "data_dir: str = \"../misc/corefBias/WinoBias/wino/data\"\n",
    "file: str = files[-1]\n",
    "file_path: str = f\"{data_dir}/{file}\"\n",
    "print(f\"File path: {file_path}\")\n",
    "\n",
    "data: dict = {\n",
    "    \"index\": [],\n",
    "    \"text\": []\n",
    "}\n",
    "with open(file_path) as f:\n",
    "    entire_data = f.readlines()\n",
    "\n",
    "for idx in range(len(entire_data)):\n",
    "    data[\"index\"].append(idx + 1)\n",
    "    data[\"text\"].append(entire_data[idx][len(str(idx)):].strip())\n",
    "\n",
    "print(data[\"index\"][100])\n",
    "print(data[\"text\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dir: str = \"../data/WinoBias/new\"\n",
    "new_file: str = file.split(\".\")[0]\n",
    "new_extension: str = file.split(\".\")[-1]\n",
    "# print(new_extension)\n",
    "# print(new_file)\n",
    "with open(f\"{new_data_dir}/{new_file}.{new_extension}.tsv\", \"w\") as f:\n",
    "    for idx in range(len(data[\"index\"])):\n",
    "        f.write(str(data[\"index\"][idx]) + \"\\t\")\n",
    "        f.write(data[\"text\"][idx] + \"\\t\")\n",
    "        if new_file[0] == 'a':\n",
    "            # Anti stereotypical\n",
    "            f.write(\"0\\n\")\n",
    "        else:\n",
    "            # Stereotypical\n",
    "            f.write(\"1\\n\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This function returns tsv files in a list format\n",
    "    - input => .tsv file\n",
    "    - output => ([texts], [labels])\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "def tsv_to_data(file_path: str) -> tuple:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for row in tsvreader:\n",
    "            text = row[1]\n",
    "            label = int(row[2])\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "\n",
    "    return (texts, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_occupations: list = [\n",
    "    \"driver\",\n",
    "    \"supervisor\",\n",
    "    \"janitor\",\n",
    "    \"cook\",\n",
    "    \"mover\",\n",
    "    \"laborer\",\n",
    "    \"construction worker\",\n",
    "    \"chief\",\n",
    "    \"developer\",\n",
    "    \"carpenter\",\n",
    "    \"manager\",\n",
    "    \"lawyer\",\n",
    "    \"farmer\",\n",
    "    \"salesperson\",\n",
    "    \"physician\",\n",
    "    \"guard\",\n",
    "    \"analyst\",\n",
    "    \"mechanic\",\n",
    "    \"sheriff\",\n",
    "    \"CEO\"\n",
    "]\n",
    "\n",
    "female_occupations: list = [\n",
    "    \"attendant\",\n",
    "    \"cashier\",\n",
    "    \"teacher\",\n",
    "    \"nurse\",\n",
    "    \"assistant\",\n",
    "    \"secretary\",\n",
    "    \"auditor\",\n",
    "    \"cleaner\",\n",
    "    \"receptionist\",\n",
    "    \"clerk\",\n",
    "    \"counselor\",\n",
    "    \"designer\",\n",
    "    \"hairdresser\",\n",
    "    \"writer\",\n",
    "    \"housekeeper\",\n",
    "    \"baker\",\n",
    "    \"accountant\",\n",
    "    \"editor\",\n",
    "    \"librarian\",\n",
    "    \"tailor\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Squeeze out the extra batch dimension\n",
    "        input_ids = encodings['input_ids'].squeeze()\n",
    "        attention_mask = encodings['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            # Convert label to float if using BCEWithLogitsLoss\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "train_texts, train_labels = tsv_to_data(file_path=\"../data/WinoBias/new/anti_stereotyped_type1.dev.tsv\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = BiasDataset(train_texts, train_labels, tokenizer, max_length=64)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanganesh/miniforge3/envs/AI/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss = 0.1963\n",
      "Epoch 2/3, Loss = 0.0261\n",
      "Epoch 3/3, Loss = 0.0116\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel, AdamW\n",
    "import torch\n",
    "\n",
    "class GenderBiasClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model='bert-base-uncased'):\n",
    "        super(GenderBiasClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = GenderBiasClassifier().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # shape: (batch_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {epoch_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_bias(model, tokenizer, sentence, device='cpu', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Given a trained model, tokenizer, and a sentence string, returns:\n",
    "      - label (0 or 1),\n",
    "      - probability (float in [0, 1]).\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits)  # shape: (batch_size,) but here batch_size=1\n",
    "\n",
    "    prob = probs.item()  # convert to float\n",
    "    label = 1 if prob >= threshold else 0\n",
    "\n",
    "    return label, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Anti-stereotypical (NO BIAS). Probability = 0.008351\n"
     ]
    }
   ],
   "source": [
    "# Suppose your new sentence is:\n",
    "new_sentence = \"The CEO raised the salary of the receptionist, because she did a good job.\"\n",
    "\n",
    "# Call the prediction function\n",
    "label, probability = predict_bias(model, tokenizer, new_sentence, device=device)\n",
    "\n",
    "# Interpret the result\n",
    "if label == 1:\n",
    "    print(f\"Prediction: Stereotypical (BIAS). Probability = {probability:.6f}\")\n",
    "else:\n",
    "    print(f\"Prediction: Anti-stereotypical (NO BIAS). Probability = {probability:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
